{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AlexNet in PyTorch\n",
    "===\n",
    "\n",
    "Most of the code are derivated from [examples/imagenet](https://github.com/pytorch/examples/tree/master/imagenet) and [torchvision/models/alexnet.py](https://github.com/pytorch/vision/blob/master/torchvision/models/alexnet.py).\n",
    "\n",
    "Paper: [ImageNet Classification with Deep Convolutional Neural Networks](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf)\n",
    "\n",
    "Slide: [AlexNet in PyTorch](https://docs.google.com/presentation/d/1EtflA3HRTNEQ5Yj5YKW6KOZ5it4HOHeDkztYbZQf-pw/edit?usp=sharing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 載入圖片資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load data\n",
    "data_dir = '.\\\\tiny-imagenet-200'\n",
    "\n",
    "traindir = os.path.join(data_dir, 'train')\n",
    "valdir = os.path.join(data_dir, 'val')\n",
    "augmentation = transforms.Compose([\n",
    "                    transforms.RandomResizedCrop(224),\n",
    "                    transforms.RandomHorizontalFlip(),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                         std=[1, 1, 1])\n",
    "               ])\n",
    "val_transform = transforms.Compose([\n",
    "                    transforms.Resize(256),\n",
    "                    transforms.CenterCrop(224),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                         std=[1, 1, 1])\n",
    "               ])\n",
    "\n",
    "train_dataset = datasets.ImageFolder(traindir, augmentation)\n",
    "val_dataset = datasets.ImageFolder(valdir, val_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 初始化 DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prepare dataloader\n",
    "batch_size = 128\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=batch_size, shuffle=True,\n",
    "        num_workers=4, pin_memory=True)\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset, batch_size=batch_size, shuffle=False,\n",
    "        num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定義網路結構"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlexNet(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 96, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): LocalResponseNorm(5, alpha=0.0001, beta=0.75, k=1)\n",
      "    (3): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Conv2d(96, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (5): ReLU()\n",
      "    (6): LocalResponseNorm(5, alpha=0.0001, beta=0.75, k=1)\n",
      "    (7): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (8): Conv2d(256, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): ReLU()\n",
      "    (10): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU()\n",
      "    (12): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU()\n",
      "    (14): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Dropout(p=0.5)\n",
      "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.5)\n",
      "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=4096, out_features=200, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class AlexNet(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes=200):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 96, kernel_size=11, stride=4, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.LocalResponseNorm(5),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(96, 256, kernel_size=5, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.LocalResponseNorm(5),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(256, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(384, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256 * 6 * 6, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(-1, 256 * 6 * 6)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "## Init network\n",
    "model = AlexNet()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function 及 Optimizer 定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameters\n",
    "lr = 0.01\n",
    "momentum = 0.9\n",
    "weight_decay = 0.0005\n",
    "\n",
    "# loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr,\n",
    "                            momentum=momentum,\n",
    "                            weight_decay=weight_decay)\n",
    "\n",
    "# Get GPU\n",
    "is_gpu = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if is_gpu else \"cpu\")\n",
    "if is_gpu:\n",
    "    model.to(device)\n",
    "    criterion.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 輔助函式\n",
    "\n",
    "`accuracy` 爲計算 top 1 及 top 5 precision\n",
    "\n",
    "`AverageMeter` 爲輔助記錄整體精確度的物件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        \n",
    "        return res\n",
    "            \n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train epoch\n",
    "def train(train_loader, model, criterion, optimizer, is_gpu, device, epoch):\n",
    "    # Metrics\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for i, (input, target) in enumerate(train_loader):\n",
    "        # Move input and target to GPU if abailable\n",
    "        if is_gpu:\n",
    "            input, target = input.to(device), target.to(device)\n",
    "\n",
    "        # Feed forward\n",
    "        output = model(input)\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # Metrics\n",
    "        prec1, prec5 = accuracy(output, target, topk=(1, 5))\n",
    "        losses.update(loss.item(), input.size(0))\n",
    "        top1.update(prec1[0], input.size(0))\n",
    "        top5.update(prec5[0], input.size(0))\n",
    "\n",
    "        # Backprop and SGD\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print every 100 batch\n",
    "        if i % 100 == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t'\n",
    "                  'Prec@5 {top5.val:.3f} ({top5.avg:.3f})'.format(\n",
    "                   epoch, i, len(train_loader), loss=losses, top1=top1, top5=top5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Validate\n",
    "def validate(val_loader, model, criterion, is_gpu, device):\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (input, target) in enumerate(val_loader):\n",
    "            if is_gpu:\n",
    "                input, target = input.to(device), target.to(device)\n",
    "\n",
    "            # compute output\n",
    "            output = model(input)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            prec1, prec5 = accuracy(output, target, topk=(1, 5))\n",
    "            losses.update(loss.item(), input.size(0))\n",
    "            top1.update(prec1[0], input.size(0))\n",
    "            top5.update(prec5[0], input.size(0))\n",
    "\n",
    "        print('VALIDATE Prec@1 {top1.avg:.3f} Prec@5 {top5.avg:.3f}'\n",
    "              .format(top1=top1, top5=top5))\n",
    "\n",
    "    return top1.avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 開始訓練！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0][0/782]\tLoss 5.2986 (5.2986)\tPrec@1 0.000 (0.000)\tPrec@5 1.562 (1.562)\n",
      "Epoch: [0][100/782]\tLoss 5.3002 (5.2985)\tPrec@1 0.000 (0.464)\tPrec@5 3.125 (2.475)\n",
      "Epoch: [0][200/782]\tLoss 5.2988 (5.2985)\tPrec@1 1.562 (0.501)\tPrec@5 3.906 (2.526)\n",
      "Epoch: [0][300/782]\tLoss 5.2985 (5.2986)\tPrec@1 2.344 (0.472)\tPrec@5 3.906 (2.385)\n",
      "Epoch: [0][400/782]\tLoss 5.2976 (5.2986)\tPrec@1 0.000 (0.470)\tPrec@5 1.562 (2.342)\n",
      "Epoch: [0][500/782]\tLoss 5.2985 (5.2986)\tPrec@1 0.781 (0.462)\tPrec@5 3.125 (2.376)\n",
      "Epoch: [0][600/782]\tLoss 5.2986 (5.2987)\tPrec@1 0.000 (0.455)\tPrec@5 2.344 (2.354)\n",
      "Epoch: [0][700/782]\tLoss 5.2993 (5.2987)\tPrec@1 0.000 (0.456)\tPrec@5 1.562 (2.310)\n",
      "VALIDATE Prec@1 0.700 Prec@5 2.500\n"
     ]
    }
   ],
   "source": [
    "## Training\n",
    "impv_prec1 = 0\n",
    "prev_prec1 = 0\n",
    "epochs = 1\n",
    "for epoch in range(epochs):\n",
    "    train(train_loader, model, criterion, optimizer, is_gpu, device, epoch)\n",
    "    prec1 = validate(val_loader, model, criterion, is_gpu, device)\n",
    "\n",
    "    impv_prec1 += prec1 - prev_prec1\n",
    "    prev_prec1 = prec1\n",
    "\n",
    "    # Check acc improvement every 10 epoch\n",
    "    if epoch % 10 == 9:\n",
    "        if impv_prec1 < 1.0:\n",
    "            lr /= 10\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr\n",
    "        print('CHECK_IMPROVE Epoch: {}, Prec1 imporve: {:.3f}, lr: {:.4f}'.format(epoch, impv_prec1, lr))\n",
    "        impv_prec1 = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 載入訓練好的 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for AlexNet:\n\tsize mismatch for features.4.weight: copying a param of torch.Size([256, 96, 5, 5]) from checkpoint, where the shape is torch.Size([128, 96, 5, 5]) in current model.\n\tsize mismatch for features.4.bias: copying a param of torch.Size([256]) from checkpoint, where the shape is torch.Size([128]) in current model.\n\tsize mismatch for features.8.weight: copying a param of torch.Size([384, 256, 3, 3]) from checkpoint, where the shape is torch.Size([256, 128, 3, 3]) in current model.\n\tsize mismatch for features.8.bias: copying a param of torch.Size([384]) from checkpoint, where the shape is torch.Size([256]) in current model.\n\tsize mismatch for features.10.weight: copying a param of torch.Size([384, 384, 3, 3]) from checkpoint, where the shape is torch.Size([256, 256, 3, 3]) in current model.\n\tsize mismatch for features.10.bias: copying a param of torch.Size([384]) from checkpoint, where the shape is torch.Size([256]) in current model.\n\tsize mismatch for features.12.weight: copying a param of torch.Size([256, 384, 3, 3]) from checkpoint, where the shape is torch.Size([128, 256, 3, 3]) in current model.\n\tsize mismatch for features.12.bias: copying a param of torch.Size([256]) from checkpoint, where the shape is torch.Size([128]) in current model.\n\tsize mismatch for classifier.1.weight: copying a param of torch.Size([4096, 9216]) from checkpoint, where the shape is torch.Size([4096, 4608]) in current model.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-66220c870e43>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m## Load model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mcheckpoint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'model_best.pth'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'state_dict'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\deep-learning\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[1;34m(self, state_dict, strict)\u001b[0m\n\u001b[0;32m    717\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    718\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[1;32m--> 719\u001b[1;33m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[0;32m    720\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for AlexNet:\n\tsize mismatch for features.4.weight: copying a param of torch.Size([256, 96, 5, 5]) from checkpoint, where the shape is torch.Size([128, 96, 5, 5]) in current model.\n\tsize mismatch for features.4.bias: copying a param of torch.Size([256]) from checkpoint, where the shape is torch.Size([128]) in current model.\n\tsize mismatch for features.8.weight: copying a param of torch.Size([384, 256, 3, 3]) from checkpoint, where the shape is torch.Size([256, 128, 3, 3]) in current model.\n\tsize mismatch for features.8.bias: copying a param of torch.Size([384]) from checkpoint, where the shape is torch.Size([256]) in current model.\n\tsize mismatch for features.10.weight: copying a param of torch.Size([384, 384, 3, 3]) from checkpoint, where the shape is torch.Size([256, 256, 3, 3]) in current model.\n\tsize mismatch for features.10.bias: copying a param of torch.Size([384]) from checkpoint, where the shape is torch.Size([256]) in current model.\n\tsize mismatch for features.12.weight: copying a param of torch.Size([256, 384, 3, 3]) from checkpoint, where the shape is torch.Size([128, 256, 3, 3]) in current model.\n\tsize mismatch for features.12.bias: copying a param of torch.Size([256]) from checkpoint, where the shape is torch.Size([128]) in current model.\n\tsize mismatch for classifier.1.weight: copying a param of torch.Size([4096, 9216]) from checkpoint, where the shape is torch.Size([4096, 4608]) in current model."
     ]
    }
   ],
   "source": [
    "## Load model\n",
    "checkpoint = torch.load('model_best.pth')\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 載入 Label 資訊"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load labels\n",
    "from collections import defaultdict\n",
    "wnids = []\n",
    "words = defaultdict(str)\n",
    "with open('wnids.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        wnids.append(line.strip('\\n'))\n",
    "wnids = sorted(wnids)\n",
    "with open('words.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        sp = line.strip('\\n').split('\\t', 1)\n",
    "        wnid = sp[0]\n",
    "        word = sp[1]\n",
    "        if wnid in wnids:\n",
    "            words[wnid] = word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 隨機挑選測試圖片"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load image and make prediction\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_id = np.random.randint(1000)\n",
    "image = Image.open(os.path.join('tiny-imagenet-200', 'test', 'images', \n",
    "                               'test_{}.JPEG'.format(rand_id)))\n",
    "plt.imshow(image)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(val_transform(image).unsqueeze(0)).numpy()[0]\n",
    "    top5_cate = output.argsort()[::-1][:5]\n",
    "    \n",
    "    for i, cate in enumerate(top5_cate):\n",
    "        print('PRED'+str(i+1)+': '+words[wnids[cate]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
