# Numpy implementation of multilayer perceptron

[Multilayer perceptron](https://en.wikipedia.org/wiki/Multilayer_perceptron)

[D. E. Rumelhart, G. E. Hinton, and R. J. Williams. 1986. Learning internal representations by error propagation. In Parallel distributed processing: explorations in the microstructure of cognition, vol. 1, David E. Rumelhart, James L. McClelland, and CORPORATE PDP Research Group (Eds.). MIT Press, Cambridge, MA, USA 318-362.](https://dl.acm.org/citation.cfm?id=104293)

Full text: https://web.stanford.edu/class/psych209a/ReadingsByDate/02_06/PDPVolIChapter8.pdf

## Other References
[Gradient Descent, Back Propagation, and Auto Differentiation - Advanced Spark and TensorFlow Meetup - 08-04-2016 - SlideShare](https://www.slideshare.net/cfregly/gradient-descent-back-propagation-and-auto-differentiation-advanced-spark-and-tensorflow-meetup-08042016)

## Implementation
1. Activation functions
    - sigmoid
2. Layers
    - weights and bias
3. Softmax
4. Loss function
5. Back-prop
    - derivation
    - chain rule
    - gradient descent
6. training
7. evaluating

A. auto differentiation
